{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc998db4",
   "metadata": {},
   "source": [
    "## Ejercicio Web Scraping bs4/Selenium/Helium\n",
    "\n",
    "_**url** = https://www.20minutos.es/_\n",
    "\n",
    "Vamos a hacer Web Scraping de las primeras 3 páginas de una de las siguientes categorias de noticias: **Ciencia**, **Deporte**, **Gente**, **Economía**, **Grastronomía** y **Opinión**.\n",
    "\n",
    "Y de cada noticia/articulo vamos a obtener:\n",
    "\n",
    "- **Titulo**\n",
    "- **Hora**\n",
    "- **Fecha**\n",
    "- **Autor**\n",
    "- **Texto completo**\n",
    "- **Categoria de la noticia**\n",
    "\n",
    "Genera un DataFrame con esta información y guardalo en el archivo **`20minutos_1.csv`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b38fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 categoría - 3 páginas\n",
    "\n",
    "# Inicializar Driver\n",
    "chrome_driver = \"../../10_10_2023_dia_18/chromedriver.exe\"\n",
    "\n",
    "url_20minutos = \"https://www.20minutos.es/\"\n",
    "\n",
    "browser = webdriver.Chrome(executable_path = chrome_driver)\n",
    "\n",
    "browser.get(url = url_20minutos)\n",
    "\n",
    "browser.maximize_window()\n",
    "sleep(2)\n",
    "\n",
    "# Aceptar Cookies\n",
    "browser.find_element_by_css_selector('#didomi-notice-agree-button > span').click()\n",
    "\n",
    "categorias = [\"Ciencia\"]\n",
    "lista_url = list()\n",
    "\n",
    "for categoria in categorias:\n",
    "    \n",
    "    # Menú de opciones\n",
    "    browser.find_element_by_css_selector('#ui-toggle-menu > a > svg.ui-unfold-menu.icon-bars > use').click()\n",
    "    sleep(1)\n",
    "    \n",
    "    browser.find_element_by_partial_link_text(link_text = categoria).click()\n",
    "    sleep(2)\n",
    "    \n",
    "    page_url = browser.current_url\n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        \n",
    "        browser.get(url = f\"{page_url}{i}\")\n",
    "    \n",
    "        # Beautiful Soup\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "        articulos = soup.find_all(\"article\", class_ = \"media\")\n",
    "\n",
    "        articulos_url = [[x.find(\"a\")[\"href\"], categoria] for x in articulos[:-4]]\n",
    "\n",
    "        lista_url.extend(articulos_url)\n",
    "        \n",
    "    browser.get(url = url_20minutos)\n",
    "    sleep(2)\n",
    "    \n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfdd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in lista_url:\n",
    "    \n",
    "    print(url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad878480",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "\n",
    "for url in lista_url:\n",
    "\n",
    "    response = requests.get(url[0])\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        titulo = soup.find(\"h1\", class_ = \"article-title\").text\n",
    "    \n",
    "    except:\n",
    "        titulo = np.nan\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        fecha, hora = soup.find(\"span\", class_ = \"article-date\").text.split(\" - \")\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        fecha, hora = np.nan, np.nan\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        autor = soup.find(\"span\", class_ = \"article-author\").text.strip()\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        autor = np.nan\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        texto = \"\\n\".join([x.text.strip() for x in soup.find_all(\"p\", class_ = \"paragraph\")])\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        texto = np.nan\n",
    "    \n",
    "    data.append([titulo, fecha, hora, autor, texto])\n",
    "    \n",
    "    sleep(1)\n",
    "    \n",
    "df = pd.DataFrame(data = data, columns = [\"titulo\", \"fecha\", \"hora\", \"autor\", \"texto\"])\n",
    "\n",
    "df[\"categoria\"] = [x[1] for x in lista_url]\n",
    "df[\"url\"] = [x[0] for x in lista_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55556d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8196be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"20minutos_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90547ae",
   "metadata": {},
   "source": [
    "- **Escribe el código para sacar la información de las primeras 5 páginas de las 6 categorias anteriores. Guarda esta información en el archivo `20minutos_2.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las categorías - 5 páginas\n",
    "\n",
    "# Inicializar Driver\n",
    "chrome_driver = \"chromedriver.exe\"\n",
    "\n",
    "url_20minutos = \"https://www.20minutos.es/\"\n",
    "\n",
    "browser = webdriver.Chrome(executable_path = chrome_driver)\n",
    "\n",
    "browser.get(url = url_20minutos)\n",
    "\n",
    "browser.maximize_window()\n",
    "sleep(2)\n",
    "\n",
    "# Aceptar Cookies\n",
    "browser.find_element_by_css_selector('#didomi-notice-agree-button > span').click()\n",
    "\n",
    "categorias = [\"Ciencia\", \"Deportes\", \"Gente\", \"Economía\", \"Gastro\", \"Opinión\"]\n",
    "lista_url = list()\n",
    "\n",
    "for categoria in categorias:\n",
    "    \n",
    "    # Menú de opciones\n",
    "    browser.find_element_by_css_selector('#ui-toggle-menu > a > svg.ui-unfold-menu.icon-bars > use').click()\n",
    "    sleep(1)\n",
    "    \n",
    "    browser.find_element_by_partial_link_text(link_text = categoria).click()\n",
    "    sleep(2)\n",
    "    \n",
    "    page_url = browser.current_url\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        \n",
    "        browser.get(url = f\"{page_url}{i}\")\n",
    "    \n",
    "        # Beautiful Soup\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "        articulos = soup.find_all(\"article\", class_ = \"media\")\n",
    "\n",
    "        articulos_url = [[x.find(\"a\")[\"href\"], categoria] for x in articulos[:-4]]\n",
    "\n",
    "        lista_url.extend(articulos_url)\n",
    "        \n",
    "    browser.get(url = url_20minutos)\n",
    "    sleep(2)\n",
    "    \n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b33b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "\n",
    "for url in lista_url:\n",
    "\n",
    "    response = requests.get(url[0])\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        titulo = soup.find(\"h1\", class_ = \"article-title\").text\n",
    "    \n",
    "    except:\n",
    "        titulo = np.nan\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        fecha, hora = soup.find(\"span\", class_ = \"article-date\").text.split(\" - \")\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        fecha, hora = np.nan, np.nan\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        autor = soup.find(\"span\", class_ = \"article-author\").text.strip()\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        autor = np.nan\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        texto = \"\\n\".join([x.text.strip() for x in soup.find_all(\"p\", class_ = \"paragraph\")])\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        texto = np.nan\n",
    "    \n",
    "    data.append([titulo, fecha, hora, autor, texto])\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "df = pd.DataFrame(data = data, columns = [\"titulo\", \"fecha\", \"hora\", \"autor\", \"texto\"])\n",
    "\n",
    "df[\"categoria\"] = [x[1] for x in lista_url]\n",
    "df[\"url\"] = [x[0] for x in lista_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"20minutos_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c7713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b802c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ad271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1c7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
