{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2488d67d",
   "metadata": {},
   "source": [
    "## Ejercicio de Clustering, Regression y Clasificacion\n",
    "\n",
    "Continuación del ejercicio anterior:\n",
    "\n",
    "Ya tenemos los modelos de regresión lineal para ambas nubes. Si quisieramos predecir una nueva instancia tendriamos que transformar esa nueva \"fila\" para que tenga el mismo \"formato\" que las filas que usamos para crear los modelos, es decir, hay que aplicar el mismo preprocesamiento para poder utilizar esa instancia con el modelo:\n",
    "\n",
    "**Parte 1**:\n",
    "1. Define una función que tome como parametro una instancia o una lista de instancias y que aplique el mismo preprocesamiento a esa instancia o lista de instancias.\n",
    "2. Esta función debe usar el mismo escalador, aplicar las mismas transformaciones y crear las mismas columnas que las realizadas en el preprocesamiento.\n",
    "3. La función debe retornar una instancia unica o una lista de instancias ya preprocesadas.\n",
    "4. Estas instancias no tendran la columna de **`CO2EMISSIONS`**.\n",
    "\n",
    "**Parte 2**:\n",
    "\n",
    "Ahora tenemos que \"clasificar\" las nuevas instancias para saber con cual modelo de regresión vamos a hacer la prediccion:\n",
    "\n",
    "1. Utilizando el dataset preprocesado de **`FuelConsumptionCo2.csv`**, vamos a realizar un modelo de clasificación para predecir la columna generada por el **`cluster`**. Es decir, el dataset para este modelo debe tener todas las columnas a excepción de **`CO2EMISSIONS`** y vamos a intentar predecir la columna **`cluster`**.\n",
    "\n",
    "2. Define una función que tome como parámetro una instancia o lista de instancias **SIN PREPROCESAR** y que haga lo siguiente:\n",
    "    - Aplique el preprocesamiento (la función definida en la **Parte 1**).\n",
    "    - Utilice el modelo de clasificación para decidir que modelo de regresión aplicar.\n",
    "    - Dependiendo del caso, que haga la predicción del modelo de regresión a cada instancia.\n",
    "    - **Debe retornar una lista de predicciones del modelo de regresión**, recuerda desescalar los datos predichos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef025a4a-0cfc-4774-ba15-b4ff36bedef3",
   "metadata": {},
   "source": [
    "# Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad297102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalizacion\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Archivos\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Train, Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metricas para regresiones\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Regresores\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Validacion\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "#Modelos qufaltan\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f22aaf-80af-4ddc-904c-799906a83d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec8e29-9a55-46ff-babb-0500e8a91edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Unnamed: 0\",\"CO2EMISSIONS\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079063c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"escaladory0.sav\", \"rb\") as file:\n",
    "    scaler_y0 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12100777-26f0-477b-9512-3eb354a5d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"escaladorX1.sav\", \"rb\") as file:\n",
    "    scaler_X1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c183e6-64a7-426f-b98e-e18b37ef73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"escaladory1.sav\", \"rb\") as file:\n",
    "    scaler_y1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e166540-e6e0-4224-9a5e-f854a944ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"escaladorX0.sav\", \"rb\") as file:\n",
    "    scaler_X0 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed01b6-6d49-4e1c-b3bc-3c1f3c16ec5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocesamiento(instancias, columnas_originales):\n",
    "    columnas = ['MODELYEAR',  'MAKE', 'MODEL', 'VEHICLECLASS', 'ENGINESIZE', 'CYLINDERS', 'TRANSMISSION',\n",
    "                'FUELTYPE', 'FUELCONSUMPTION_CITY', 'FUELCONSUMPTION_HWY', 'FUELCONSUMPTION_COMB', 'FUELCONSUMPTION_COMB_MPG']\n",
    "\n",
    "    df_nuevo= pd.DataFrame(instancias, columns=columnas)\n",
    "    df_nuevo.drop([\"MODELYEAR\", \"MAKE\", \"MODEL\"], axis = 1, inplace= True)\n",
    "    df_num = df_nuevo._get_numeric_data().copy()\n",
    "    df_cat = df_nuevo.drop(df_num.columns, axis = 1)\n",
    "    \n",
    "    #Procesa Bikerclass Class (Reducir valores y luego a numerica)\n",
    "    dict_class = {y:x.split(\" - \")[0] for x, y in zip (df_cat[\"VEHICLECLASS\"].unique(), df_cat[\"VEHICLECLASS\"].unique())}\n",
    "    df_cat[\"VEHICLECLASS\"] = df_cat[\"VEHICLECLASS\"].map(dict_class)\n",
    "    dict_class = {\"SUBCOMPACT\":\"COMPACT\", \"MINICOMPACT\":\"COMPACT\",\"MINIVAN\":\"VAN\"}\n",
    "    df_cat[\"VEHICLECLASS\"] = df_cat[\"VEHICLECLASS\"].replace(dict_class)\n",
    "    df_cat = pd.concat([df_cat, pd.get_dummies(data = df_cat[\"VEHICLECLASS\"], prefix = \"VEHICLECLASS\")], axis = 1)\n",
    "    df_cat.drop(\"VEHICLECLASS\", axis = 1, inplace= True)\n",
    "    \n",
    "    # Procesar la trasmision para dividirla en 2 coluimnas (Reducir valores y luego a numerica)\n",
    "    df_cat[\"TRANSMISSION\"] = df_cat[\"TRANSMISSION\"].apply(lambda x : \"AV0\" if x == \"AV\" else x)\n",
    "    df_cat[\"MARCHAS\"] = df_cat[\"TRANSMISSION\"].apply(lambda x : x [-1])\n",
    "    df_cat[\"TRANSMISSION\"] = df_cat[\"TRANSMISSION\"].apply(lambda x : x [:-1])\n",
    "    df_cat = pd.concat([df_cat, pd.get_dummies(data = df_cat[\"TRANSMISSION\"], prefix = \"TRANSMISSION\")], axis = 1)\n",
    "    # Procesamiento de Fulltype\n",
    "    df_cat = pd.concat([df_cat, pd.get_dummies(data = df_cat[\"FUELTYPE\"], prefix = \"FUELTYPE\")], axis = 1)\n",
    "    df_cat.drop([\"TRANSMISSION\", \"FUELTYPE\"], axis=1, inplace=True)\n",
    "    \n",
    "    #Eliminamos  Es Duplicada y concatenamos las valores categoricas\n",
    "    df_num.drop(\"FUELCONSUMPTION_COMB\", axis=1, inplace=True)\n",
    "    df_nuevo = pd.concat([df_cat, df_num], axis = 1)\n",
    "    \n",
    "    #Filtrados\n",
    "    df_nuevo = df_nuevo[df_nuevo[\"FUELCONSUMPTION_COMB_MPG\"] < 52]\n",
    "    df_nuevo [\"FUELCONSUMPTION_COMB_MPG_2\"]= df_nuevo[\"FUELCONSUMPTION_COMB_MPG\"].apply(lambda x: x**-1)\n",
    "    \n",
    "    #Eliminamos la columna original\n",
    "    df_nuevo.drop([\"FUELCONSUMPTION_COMB_MPG\"], axis=1, inplace=True)\n",
    "    df_nuevo.reset_index(drop=True)\n",
    "    \n",
    "    # NORMALIZACION\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Es para que se incopore las columas dumes (Las creadas de 1 & 0 a partir de los valores) del dataframe original que no esta en el dataframe nuevo por no tener valores\n",
    "    df_original = pd.DataFrame(columns = columnas_originales)\n",
    "    df_nuevo = pd.concat([df_original, df_nuevo]).fillna(0)\n",
    "    df_nuevo.drop([\"cluster_dbscan\"], axis=1, inplace=True)\n",
    "    \n",
    "    return df_nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75723a21-5da7-48cc-abcf-9f6b79eed1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo = prepocesamiento([[2014,\"ACURA\",\"ILX\",\"MINICOMPACT\",1.5,4,\"M8\",\"X\",6.0,5.8,5.9,48],\n",
    " [2014,\"ACURA\",\"ILX HYBRID\",\"MINICOMPACT\",1.5,4,\"AV7\",\"Z\",6.0,5.8,5.9,48]], df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370dc35-6694-4713-a5c5-b8a39be8e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f9cae-ef0e-4370-b27c-d4af49276191",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.DataFrame(columns = df.columns)\n",
    "pd.concat([df_original, ejemplo]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de0c08-e829-4c2a-a5cb-fa8cb98100ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"cluster_dbscan\"]!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85704f4d-5384-4076-9cc6-90ae19f98104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(df.drop(\"cluster_dbscan\", axis=1))\n",
    "y = np.asarray(df[\"cluster_dbscan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40015b06-d2a9-4237-9071-d60f90c93ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 10)\n",
    " \n",
    "print(f\"Conjunto de Train: {X_train.shape, X_test.shape}\")\n",
    "print(f\"Conjunto de Test: {y_train.shape, y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc52fa-8ec4-42bd-a6b6-dae910ce78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hecho_encasa(vecinos,radio):\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = vecinos)\n",
    "\n",
    "    cercanos=model.fit(X_train, y_train)\n",
    "    yhat1 = model.predict(X_test)\n",
    "\n",
    "    print(\"KNeighborsClassifier: ROC AUC:\", roc_auc_score(y_test, yhat1))\n",
    "    \n",
    "\n",
    "    print (\"*\"*100)\n",
    "    \n",
    "    model = RadiusNeighborsClassifier(radius = radio, outlier_label = \"most_frequent\")\n",
    "    radios_vecinos=model.fit(X_train, y_train)\n",
    "    yhat2 = model.predict(X_test)\n",
    "\n",
    "    print(\"RadiusNeighborsClassifier: ROC AUC:\", roc_auc_score(y_test, yhat2))\n",
    "    \n",
    "    print (\"*\"*100)\n",
    "    \n",
    "    model = NearestCentroid(metric = \"euclidean\")\n",
    "    centroides=model.fit(X_train, y_train)\n",
    "    yhat3 = model.predict(X_test)\n",
    "\n",
    "    print(\"NearestCentroid: ROC AUC:\", roc_auc_score(y_test, yhat3))\n",
    "    \n",
    "    print (\"*\"*100) \n",
    "    \n",
    "    model = GaussianNB()\n",
    "    bayes = model.fit(X_train, y_train)\n",
    "    yhat4 = model.predict(X_test)\n",
    "\n",
    "    print(\"GaussianNB: ROC AUC:\", roc_auc_score(y_test, yhat4))\n",
    "    \n",
    "    print (\"*\"*100)  \n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    arbol_decision = model.fit(X_train, y_train)\n",
    "    yhat5 = model.predict(X_test)\n",
    "    \n",
    "    print(\"DecisionTreeClassifier: ROC AUC:\", roc_auc_score(y_test, yhat5))\n",
    "    \n",
    "    print (\"*\"*100) \n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    bosque = model.fit(X_train, y_train)\n",
    "    yhat6 = model.predict(X_test)\n",
    "    \n",
    "    print(\"RandomForestClassifier: ROC AUC:\", roc_auc_score(y_test, yhat6))\n",
    "    \n",
    "    print (\"*\"*100)\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    logistic = model.fit(X_train, y_train)\n",
    "    yhat7 = model.predict(X_test)\n",
    "    \n",
    "    print(\"LogisticRegression: ROC AUC:\", roc_auc_score(y_test, yhat7))\n",
    "    \n",
    "    print (\"*\"*100)\n",
    "    \n",
    "    \n",
    "    #Adabust \n",
    "    model = AdaBoostClassifier()\n",
    "    logistic = model.fit(X_train, y_train)\n",
    "    yhat8 = model.predict(X_test)\n",
    "    \n",
    "    print(\"AdaBoostClassifier: ROC AUC:\", roc_auc_score(y_test, yhat8))\n",
    "    \n",
    "    print (\"*\"*100)\n",
    "    \n",
    "    \n",
    "    #Gradient \n",
    "    \n",
    "    model = GradientBoostingClassifier()\n",
    "    logistic = model.fit(X_train, y_train)\n",
    "    yhat9 = model.predict(X_test)\n",
    "    \n",
    "    print(\"GradientBoosting: ROC AUC:\", roc_auc_score(y_test, yhat9))\n",
    "    \n",
    "\n",
    "    print (\"*\"*100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Voting-Clasifaller FALTAN Parametros\n",
    "    \n",
    "    #model = VotingClassifier()\n",
    "    #logistic = model.fit(X_train, y_train)\n",
    "    #yhat9 = model.predict(X_test)\n",
    "    \n",
    "    #print(\"GradientBoosting: ROC AUC:\", roc_auc_score(y_test, yhat9))\n",
    "    \n",
    "\n",
    "    #print (\"*\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7b892-4d49-4f26-90d5-9d30419e8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hecho_encasa(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a8657-926f-4db9-866c-33bebc583b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ec861-da98-40b1-9dc7-d49e7d8aa590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elegido Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab3fd0-ba1b-437d-9948-5b7f91d8f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57396ba9-2bd3-4fb5-bcff-62aec5e29974",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_clasificador = GradientBoostingClassifier()\n",
    "modelo_clasificador.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae976dc-3c5e-49cf-a61b-960d54f0c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paso 1\n",
    "\n",
    "nueva_instancia = [[2014,\"ACURA\",\"ILX\",\"MINICOMPACT\",1.5,4,\"M8\",\"X\",6.0,5.8,5.9,48]]\n",
    "nueva_instancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66216d2a-c61b-4ebd-969f-2284062e7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_instancia = prepocesamiento(nueva_instancia, df.columns)\n",
    "nueva_instancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab469223-24bd-4341-928d-6da712cea202",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_clasificador.predict(nueva_instancia)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffcf33-93d9-4538-bab4-986131624ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def todo_el_modelo(nueva_instancia):\n",
    "    \n",
    "    nueva_instancia = prepocesamiento(nueva_instancia, df.columns)\n",
    "    \n",
    "    resultado_clasificacion = modelo_clasificador.predict(nueva_instancia.values)[0]\n",
    "    \n",
    "    \n",
    "    if resultado_clasificacion == 0:\n",
    "        with open(\"modelo_grupo0.sav\", \"br\") as file:\n",
    "            modelo = pickle.load(file)\n",
    "        nueva_instancia[\"cluster_dbscan\"] = 0\n",
    "        \n",
    "        with open(\"escaladorX0.sav\", \"br\") as file:\n",
    "            escaladorX0 = pickle.load(file)\n",
    "            \n",
    "        nueva_instancia = escaladorX0.transform(nueva_instancia.values)\n",
    "        \n",
    "        resultado_regresion = modelo.predict(nueva_instancia)\n",
    "   \n",
    "        with open(\"escaladory0.sav\", \"br\") as file:\n",
    "            escaladory0 = pickle.load(file)\n",
    "            \n",
    "        resultado_regresion = escaladory0.inverse_transform(resultado_regresion)\n",
    "\n",
    "   \n",
    "    else:\n",
    "        with open(\"modelo_grupo1.sav\", \"br\") as file:\n",
    "            modelo = pickle.load(file)\n",
    "        nueva_instancia[\"cluster_dbscan\"] = 1\n",
    "        \n",
    "        with open(\"escaladorX1.sav\", \"br\") as file:\n",
    "            escaladorX1 = pickle.load(file)\n",
    "            \n",
    "        nueva_instancia = escaladorX1.transform(nueva_instancia.values)\n",
    "        \n",
    "        resultado_regresion = modelo.predict(nueva_instancia)\n",
    "        \n",
    "        with open(\"escaladory1.sav\", \"br\") as file:\n",
    "            escaladory1 = pickle.load(file)\n",
    "            \n",
    "        resultado_regresion = escaladory1.inverse_transform(resultado_regresion)\n",
    "        \n",
    "        \n",
    "    return resultado_regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8624f0d-6c0a-4d82-9b2d-2ef6709af6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_instancia = [[2014,\"ACURA\",\"ILX\",\"COMPACT\",2.0,4,\"AS5\",\"Z\",9.9,6.7,8.5,33]]\n",
    "\n",
    "todo_el_modelo(nueva_instancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b838fb4-85e0-44b1-a13b-0c1c62134c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187dc20-e047-4fe3-be4a-3ef94ffc7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sin = pd.read_csv(\"FuelConsumptionCo2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0897b4-6a87-4c32-97b9-c52e5a4bbd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevas_instancias = prepocesamiento(df_sin.iloc[df.index, :], df.columns)\n",
    "\n",
    "nuevas_instancias.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36ad95-184b-4362-bfa0-8c930b0dfa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = todo_el_modelo(nueva_instancia = df_sin.iloc[nuevas_instancias.index, :])\n",
    "resultados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6effbca-93d2-42be-85e2-a580f6118ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados = pd.DataFrame()\n",
    "\n",
    "df_resultados[\"real\"] = df_sin.iloc[nuevas_instancias.index, :][\"CO2EMISSIONS\"]\n",
    "\n",
    "df_resultados[\"prediccion\"] = resultados\n",
    "\n",
    "df_resultados[\"diferencia\"] = abs(df_resultados[\"prediccion\"] - df_resultados[\"real\"])\n",
    "\n",
    "df_resultados.sort_values(\"diferencia\").head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07acac47-e6eb-48ae-882e-b964aaeeee0e",
   "metadata": {},
   "source": [
    "# 3 Parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb094f-dba9-4a76-a214-a07d366c5337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
