{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0c773e",
   "metadata": {},
   "source": [
    "## Ejercicio NLP & WebScraping\n",
    "\n",
    "1. Hacer Web Scraping sobre los siguientes links para obtener los diálogos de cada capítulo de la serie:\n",
    "\n",
    "- https://bigbangtrans.wordpress.com/series-1-episode-1-pilot-episode/\n",
    "- https://bigbangtrans.wordpress.com/series-1-episode-2-the-big-bran-hypothesis/\n",
    "- https://bigbangtrans.wordpress.com/series-1-episode-3-the-fuzzy-boots-corollary/\n",
    "- https://bigbangtrans.wordpress.com/series-1-episode-4-the-luminous-fish-effect/\n",
    "- https://bigbangtrans.wordpress.com/series-1-episode-5-the-hamburger-postulate/\n",
    "\n",
    "2. Una vez que se obtuvieron los diálogos de cada capítulo, unifacamos los textos de cada capítulo en uno solo.\n",
    "3. El primer paso es separar los diálogos por personaje (Sheldon, Leonard, Penny, Raj, Howard). \n",
    "4. Tokenizar cada dialogo por personaje con la función **`.word_tokenize()`**. \n",
    "5. Eliminar stopwords y signos de puntuación (. , : ; ¡ ¿ ? …) de las listas con tokens\n",
    "6. Encontrar la riqueza léxica de cada personaje\n",
    "7. Encontrar las 3 palabras más utilizadas por cada personaje\n",
    "8. Encontrar palabras similares para la palabra más utilizada de cada personaje\n",
    "9. Encontrar hápax de cada personaje\n",
    "10. Utilizar la función nltk.FreqDist() para mostrar en una gráfica las 20 palabras que más se utilizan.\n",
    "11. Repetir el ejercicio con todos los episodios de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a44882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023ee7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba6009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb137f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb5519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b8080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b128e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e936f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd7db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728adb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b8760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
